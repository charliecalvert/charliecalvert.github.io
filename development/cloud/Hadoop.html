<!DOCTYPE html>
<html dir="ltr">

<!-- #BeginTemplate "../../Elvenware.dwt" -->

<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
<!-- #BeginEditable "doctitle" -->
<title>Hadoop</title>
<meta name="description" content="Some documentation on installing Hadoop" />
<meta name="keywords" content="hadoop, install" />
<!-- #EndEditable -->
<meta content="width=device-width,minimum-scale=1.0,maximum-scale=1.0" name="viewport" />
<link href="/charlie/libs/css/style2.css" rel="stylesheet" title="style2" type="text/css" />
<link href="/charlie/libs/jquery-ui/jquery-ui.css" rel="stylesheet" type="text/css" />
<script src="/charlie/libs/scripts/jquery.min.js" type="text/javascript"></script>
<script src="http://ajax.googleapis.com/ajax/libs/jqueryui/1.10.3/jquery-ui.min.js" type="text/javascript"></script>
<script src="/charlie/libs/scripts/elvenware.js" type="text/javascript"></script>
<!-- #BeginEditable "docScripts" -->
<style type="text/css">




.auto-style1
{
	text-align: left;
}

</style>
<!-- #EndEditable -->
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-2806409-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</head>

<body>

<div id="container">
	<header>
		<div class="navButton" onclick="elvenware.toggleMenu()">
			Toggle Menu</div>
		<h1>Charlie Calvert on Elvenware</h1>
		<h2>Writing Code and Prose on Computers</h2>
	</header>
	<nav>
		<h2>Menu</h2>
		<div id="accordion00">
		<h2 class="navSection01">Core Code</h2>
			<ul>
				<li><a href="../index.html">Strongly Typed</a></li>
				<li><a href="../web/index.html">Web &amp; Scripts</a></li>
				<li><a href="index.shtml">Cloud</a></li>
			</ul>
		
			<h2 class="navSection02">OS and Tools</h2>
			<ul>
				<li><a href="../../os/index.html">OS</a></li>
				<li><a href="../database/index.html">Database</a></li>
				<li><a href="../../books/index.html">My Writing</a></li>
			</ul>
			
		
			<h2 class="navSection03">Art</h2>
			<ul>
				<li><a href="../../Art/index.html">Poems &amp; Photos</a></li>
				<li><a href="../../books/reading/index.html">Book Reviews</a></li>
				<li><a href="../../spirit/index.html">Spiritual</a></li>
			</ul>
			<h2 class="navSection04">Links</h2>
			<ul>
				<li><a href="../../links.html">My Links</a></li>
				<li><a href="http://www.falafel.com/">Falafel</a></li>
				<li><a href="http://sourceforge.net/projects/elvenware/">Sourceforge</a></li>
			</ul>
		</div>
	</nav>
	<article>
		<figure>
			<img alt="Elvenware" src="../../images/elvenwarelogo.png" /></figure>
		<!-- #BeginEditable "body" -->


<h2>Index</h2>
		<ul>
			<li><a href="#what">What is Hadoop</a></li>
			<li><a href="#mapReduce">MapReduce and HDFS</a></li>
			<li><a href="#mercurial">The Elvenware Mercurial Repository Hadoop 
			Setup Scripts</a></li>
			<li>Install <a href="#java">Java</a></li>
			<li><a href="#user">Set up a User Called Hadooper</a></li>
			<li>Install and setup <a href="#ssh">SSH</a></li>
			<li>Install <a href="#hadoop">Hadoop</a></li>
			<li><a href="#environment">Set up the Environment</a></li>
			<li><a href="#configure">Configure Hadoop</a></li>
			<li><a href="#startHadoop">Start Hadoop</a></li>
			<li><a href="#restart">Restart Hadoop</a></li>
			<li><a href="#multinode">Multinode</a></li>
			<li><a href="#runTest">Running a Test Application</a></li>
			<li><a href="#scripts">Understanding the Scripts</a></li>
			<li><a href="#links">Links</a></li>
		</ul>
		<h2 id="what">What is Hadoop</h2>

<p>A typical hard drive is 2 terabytes (2 TB) in size. It takes about five hours 
to read all the data on a single disk. If we could split the data over 200 
hard drives, all 2 TB could be read in about 5 minutes. This is possible because 
each drive contains only 1/200th of the data, and thus needs to read much less 
data to finish its part of the task. It is the same principle that is involved 
with asking 1 person with a single gallon bucket to move a 2000 gallons of water 
vs. asking 200 people with 200 one gallon buckets to move 2000 gallons of water. 
If a trip takes one minute, then one person can do the job in 2000 minutes or 33 
hours. 200 people, on the other hand, can do the job in 10 minutes. This is the power of distributed, parallel 
computing, and it is the task undertaken by Hadoop.</p>

<p>One of the goals of Hadoop is to make it possible to do distributed data over 
multiple drives bought at a typical computer store. We are not talking about some 
huge mainframe, but about off the shelf drives you might buy at Amazon, Newegg 
or at Fry's Electronics.</p>

<p>Hadoop is designed to solve problems associated with huge datasets. If you a 
site gets 100 million visitors each month, each of which leaves a trail of data 
one megabyte in size, then the site must track 100 million megabytes a month, or 
over a billion megabytes a year. </p>

<p>Hadoop was partially developed at Google, and they use it to solve many of 
their problems.</p>
		<h2 id="mapReduce">MapReduce and HDFS</h2>

<p>If we are going to distribute data over 100 hard drives, then we need to find a 
way to reassemble it, or to combine it with other bits of data quickly and 
easily. The part of Hadoop called MapReduce addresses this issue. Map/Reduce 
assembles the bits of data across the various hard drives, but it does more than 
that. It allows developers to create applications that run concurrently across 
all the nodes of the system, with one set of tasks performed on one machine, 
another on a second machine and so on. That is the Map part of the equation. 
Then the reduce part of the equation takes all these distributed pieces of work 
and assembles the results.</p>

<p>Suppose ten million visitors</p>

<p>The part of Hadoop called HDFS deals with the problem of hardware failure. If 
we are going to spread out data over 100 hard drives, then the chance of loosing 
some of it due to hardware failure is higher than it might be on a single 
machine. The goal is to duplicate the data, so a loss of one hard drive will not 
destroy the entire terabyte of data.</p>

<h2>Setup</h2>
<p>Most of the rest of this document describes one possible way to set up Hadoop 
with a minimal configuration for testing Hadoop on a single machine. Hadoop 
normally runs in multi-user mode, but here we are going to use only a single 
machine. This kind of setup provides a good way for you to learn the basics of 
hadoop before you begin deployment across multiple machines.</p>

		<h3 id="mercurial">Mercurial</h3>
<p>I have created a set of scripts that can help step you through the Hadoop 
install. They are stored with a number of other files and programs in the 
Elvenware repository. Instructions on setting up Mercurial are now on the Mercurial page:</p>
		<pre class="code"><a href="http://www.elvenware.com/charlie/development/cloud/Mercurial.html">http://www.elvenware.com/charlie/development/cloud/Mercurial.html</a></pre>
		<h3 id="java">Install Java</h3>

<p>Setting up Java can be a bit of a problem. It is best to get it out of the 
way first. I have tested with both Sun JDK6 and Sun JDK7, and both seem to work. 
Below I focus mosting on JDK6, but here is a post on
<a href="http://askubuntu.com/questions/55848/how-do-i-install-oracle-java-jdk-7" target="_blank">
installing JDK7</a>.</p>
        <p class="note">NOTE: I have recently come to rely on the simple method found here:</p>
        <pre><a class="code" href="http://www.webupd8.org/2012/01/install-oracle-java-jdk-7-in-ubuntu-via.html">http://www.webupd8.org/2012/01/install-oracle-java-jdk-7-in-ubuntu-via.html</a></pre>
        <p>By default, you do not have the right version of Java on Ubuntu Linux. The 
following commands should fix the situation:</p>

<pre class="code">wget https://raw.github.com/flexiondotorg/oab-java6/master/oab-java6.sh -O oab-java6.sh
chmod +x oab-java6.sh
sudo ./oab-java6.sh
</pre>

<p>After the endless process outlined above finally terminates, do this:</p>

<pre class="code">
sudo apt-get install sun-java6-jdk
sudo update-alternatives --config java</pre>

<p>More information on the scripts shown above is available here:</p>

<p><a href="https://github.com/flexiondotorg/oab-java6">
https://github.com/flexiondotorg/oab-java6</a></p>

<p>This should work to setup Java Sun on Mint Linux:</p>
		<pre class="code">sudo add-apt-repository "deb http://archive.canonical.com/ lucid partner"
sudo apt-get update
sudo apt-get install sun-java6-jdk
sudo update-java-alternatives -s java-6-sun</pre>
		<p>On one of my system, a day or more after completing&nbsp; the above 
		steps, when I ran <strong>sudo apt-get update</strong> I got the following error:</p>
		<pre class="code">W: GPG error: http://ppa.launchpad.net oneiric Release: The following signatures 
couldn't be verified because the public key is not available: NO_PUBKEY 2EA8F35793D8809A</pre>
		<p>To elimate the error, I used the menu to go to <strong>System 
		Settings | Software Sources | Other Software</strong> and unchecked<strong> </strong>
		the references to flexiondotorg. Then I could run <strong>sudo apt-get 
		update</strong> without error.</p>

		<h3 id="user">User Setup</h3>

<p>It is optional as to whether or not to create a user called Hadoop, but I 
strongly recommend that you do so. This may have security benefits, but we like it 
primarily
because it keeps all the Hadoop related configuration in one place. In the 
process a custom .<strong>bashrc</strong>
file is set up just for the Hadoop user, which means that we can configure the environment 
just for him withou muddying up the configuration your primary user 
account, which can be nice.</p>

<p>This code create a new group called hadoop, adds a user to it, adds the user
to the admins group and then switches you over from being yourself to being
hadooper. There will be a number of prompts you need to respond to on the way.</p>

<pre class="code">sudo addgroup hadoop
sudo adduser --ingroup hadoop hadooper
sudo usermod -a -G admin hadooper
su -l hadooper</pre>

		<p>I have created a script called <strong>CreateUser.sh</strong> that 
		performs these actions automatically. See the <a href="#scripts">end of 
		this document </a>for more details.<br>
</p>
		<h3 id="ssh">SSH Setup</h3>
		<p>For Hadoop to work, you have to be able to use SSH to communicate 
		between two machines. In some cases, you will even need to be able to 
		SSH into your current machine. There is generally no real world use for 
		using SSH to communicate with your own machine, but sometimes it can be 
		part of a testing process. In particular, if you are setting up HADOOP 
		to run on a single node for testing or educational purposes, then you 
		will need to SSH into your current machine. I cover these aspects of SSH 
		in a separate file:</p>
		<ul>
			<li><a href="SshFtpsPutty.html#sshKeys">Documentation on SSH and the <strong>authorized_keys</strong> file.</a></li>
		</ul>
<p>Read the appropriate documentation on SSH and then return to this document 
after you have learned how to use SSH to start an SSH session on your localhost.</p>

		<h2 id="hadoop">Install Hadoop</h2>
<p>Assuming you have completed the above steps, you are now ready to install 
Hadoop. This can be accomplished by running the <strong>addNano.sh </strong>
script discussed at <a href="#scripts">the end of this document</a>. What 
follows is a description of that script does.</p>

		<p>Here is the download page for Hadoop:</p>
<pre class="code"><a href="http://www.apache.org/dyn/closer.cgi/hadoop/common/" target="_blank">http://www.apache.org/dyn/closer.cgi/hadoop/common/</a></pre>
<p>You can also try to download Hadoop by issuing the following command at the 
Linux command prompt:</p>
<pre class="code">wget http://apache.cs.utah.edu//hadoop/common/hadoop-1.0.1/hadoop-1.0.1.tar.gz</pre>
<p>The following script downloads and extracts hadoop:</p>
		<pre class="code
		">wget http://apache.cs.utah.edu//hadoop/common/hadoop-1.0.1/hadoop-1.0.1.tar.gz
tar xzf hadoop-1.0.1
sudo mv hadoop-1.0.1.tar.gz /usr/local/hadoop
sudo chown -R hadooper:hadoop /usr/local/hadoop
</pre>
		<p>&nbsp;The scripts starts by downloading a file that is both zipped (gz) and tarred (.tar):</p>

		<pre class="code">hadoop-1.0.1-bin.tar.gz</pre>

<p>Take a moment to examine this file name, and particular the part at the end. The 
<strong>tar</strong> extensions means that many files have been wrapped together in one 
big file called <strong>hadoop-1.0.1-bin.tar</strong>. Then gzip compressess that file into a file called 
<strong>hadoop-1.0.1.tar.gz</strong>. The following command reverses the process by unzipping the tar file and then extracting (untarring) the contents:</p>

		<pre class="code">tar xzf hadoop-1.0.1-bin.tar.gz </pre>

<p>After the command is run you should see a folder called hadoop-1.0.1. You can usually tell a folder from a file because it is shown in light blue, and because its permissions begin with drwxr-etc.</p>

<p>The script then moves (mv) the folder to /usr/local/hadoop. The command does two things: it moves the folder to a new location, and then renames it by removing the version number. When you are done, you should be able to see the contents of the folder:</p>

<pre class="code">charlie@MintBox ~/Downloads $ ls /usr/local/hadoop/
bin hadoop-client-1.0.1.jar ivy.xml sbin
build.xml hadoop-core-1.0.1.jar lib share
c++ hadoop-examples-1.0.1.jar libexec src
CHANGES.txt hadoop-minicluster-1.0.1.jar LICENSE.txt test.sh
conf hadoop-test-1.0.1.jar logs webapps
contrib hadoop-tools-1.0.1.jar NOTICE.txt
hadoop-ant-1.0.1.jar ivy README.txt
charlie@MintBox ~/Downloads $ 
</pre>
		<h2 id="environment">Set up the Environment</h2>

		<p>It is now time to set up the environment. There are two useful, but 
		optional, environment variables that we can set up, plus we must set up JAVA_HOME. 
		To set up these ennvironment variables, we could type the following each 
		time we become Hadooper:</p>
		<pre class="code">export JAVA_HOME=/usr/lib/jvm/default-java
</pre>


<p>The above is preferred, but alternatively, you can explicitly name the version you want to use:</p>
        <pre class="code">export JAVA_HOME=/usr/lib/jvm/java-6-sun</pre>
        <p>
            Rather than trying to configure these items by hand each time we become 
Hadooper, it is better to put them in a file called <strong>.bashrc</strong>. To 
oversimplify a somewhat complex subject, I'll say only that the <strong>.bashrc</strong> file 
allows us to configure the environment 
automatically each time we log in to the bash shell. This occurs because 
<strong>.bashrc</strong> is run once, just as we are signing in as hadooper. You 
will recall that to sign in as hadooper, we usually write something like this:
<strong>su - hadooper</strong>. </p>

		<p>NOTE: <em>Files that begin with a period are "invisible" or "hidden" by 
		default. If we type </em> <strong><em>ls</em></strong><em> to get a listing of a directory, 
		we don't see them. To make them "visible," we should type</em> <strong>ls -</strong>a:</p>

		<pre class="code">$ <strong>ls</strong>
andelf bar Downloads examples.desktop
$ <strong>ls -a</strong>
. bar <strong>.bashrc</strong> examples.desktop .sudo_as_admin_successful .. 
.bash_history .cache .profile andelf .bash_logout Downloads .ssh</pre>

		<p>Here is the code we want to put in the .<strong>bashrc</strong> file:</p>

<pre class="code"># Set JAVA_HOME:
export JAVA_HOME=/usr/lib/jvm/default-java
</pre>

<p>The first line is a comment, the second sets a variable in the environment of 
our OS. Our goal, then, is to create a file called .<strong>bashrc</strong> in our HOME directory, which will be 
<strong>/home/hadooper</strong>. Type <strong>cd</strong> followed by enter with no parameters to move to your home directory, and then to check which directory you are in right now, type 
<strong>pwd</strong>:</p>

<pre class="code">hadooper@WesternSeas:~$/bin cd
hadooper@WesternSeas:~$ pwd
/home/hadooper</pre>

<p>As you can see, we are now in <strong>hadooper's</strong> home directory, 
which is <strong>/home/hadooper. </strong>That's just where we want to be.</p>

		<p>The home directory is where you want to create your .<strong>bashrc</strong> file. To create it, type:</p>

<pre class="code">nano .bashrc</pre>

<p>You may find that .<strong>bashrc</strong> already exists, and already has the right entries in it, as one of my scripts takes care of that for you. But if you want to do it all by hand, then you should enter the 
following code into .<strong>bashrc</strong>, then type <strong>Ctrl-O</strong> 
plus <strong>enter</strong> to save and <strong>Ctrl-X</strong> to exit:</p>

<pre class="code"># Set JAVA_HOME:
export JAVA_HOME=/usr/lib/jvm/default-java
</pre>

<p>At this stage we need to run the .bashrc file so that the environment will be 
properly set up. The best way to run the file is to temporarilly stop being 
hadooper, and then sign back in again. To do this, type exit once or twice, until you become 
the regular user again, that is, until you exit the <strong>hadooper</strong> shell. Then log back in as hadooper:</p>

<pre class="code">su - hadooper</pre>

<p>When you log back in your .<strong>bashrc</strong> file will run automatically, as indeed it will each time you log in 
from this point forward. 
That's the whole point of .<strong>bashrc</strong>, it is a file that is run when you log into 
the bash shell; 
you place in that file any code that you want to run at the beginning of a user 
session. The type of code we placed in .<strong>bashrc</strong> is typical, in that it is designed 
to set up the environment. In particular, when your .<strong>bashrc</strong> file runs, 
<strong>JAVA_HOME</strong> variables in the environment. To check this, type echo $<strong>JAVA_HOME</strong>, etc:</p>

<pre class="code">echo $JAVA_HOME
/usr/lib/jvm/java-6-sun
echo $PATH
/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:<strong>/usr/local/hadoop/bin</strong></pre>

		<p>As you can see, all the environment variables that we wanted to 
		configure are now set up correctly. 
</p>
        <p>Alternatively, the following can be used to test the status of an environment variable:</p>
        <pre class="code">{$JAVA_HOME:?}</pre>
        <p>
            We are ready to move on to the next 
		step.<br>
        </p>





		<h2 id="configure">Configure Hadoop</h2>
<p>There are several files found in the <strong>conf </strong>directory that you 
need to configure:</p>
<ul>
	<li><strong>Common properties</strong>: core-site.xml</li>
	<li><strong>Configure HDFS</strong>: hdfs-site.xml</li>
	<li><strong>Configure MapReduce</strong>: mapred-site.xml</li>
</ul>
<p>And here is what goes in each file:</p>
<pre>&lt;?xml version="1.0"?&gt; 
&lt;!-- core-site.xml --&gt; 
&lt;configuration&gt; 
  &lt;property&gt; 
    &lt;name&gt;fs.default.name&lt;/name&gt; 
    &lt;value&gt;hdfs://localhost/&lt;/value&gt; 
  &lt;/property&gt; 
&lt;/configuration&gt; 

&lt;?xml version="1.0"?&gt; 
&lt;!-- hdfs-site.xml --&gt; 
&lt;configuration&gt; 
  &lt;property&gt; 
    &lt;name&gt;dfs.replication&lt;/name&gt; 
    &lt;value&gt;1&lt;/value&gt; 
  &lt;/property&gt; 
&lt;/configuration&gt; 

&lt;?xml version="1.0"?&gt; 
&lt;!-- mapred-site.xml --&gt; 
&lt;configuration&gt; 
  &lt;property&gt; 
    &lt;name&gt;mapred.job.tracker&lt;/name&gt; 
    &lt;value&gt;localhost:8021&lt;/value&gt; 
  &lt;/property&gt;
&lt;/configuration&gt;</pre>

		<p>Also be sure you set up the tmp directory and format the namenode:</p>
		<pre class="code">sudo mkdir -p /app/hadoop/tmp
sudo chown -R hadooper:hadoop /app/hadoop/tmp
/usr/local/hadoop/bin/hadoop namenode -format
</pre>



<h2 id="startHadoop">Starting Hadoop</h2>
		<p><em><strong>This section under construction....</strong></em></p>

		<p>To start Hadoop, run the scripts in the bin directory:</p>
		<pre class="code">/usr/local/hadoop/bin</pre>
		<p>Assuming you have navigated to the bin directory, the command to 
		start Hadoop looks liks this:</p>
		<pre class="code">./start-all.sh </pre>

		<p>To see if Hadoop started correctly, use the jps command. The output 
		should look something like this:</p>

		<pre class="code">$ jps
3216 DataNode
3524 JobTracker
3917 Jps
3759 TaskTracker
2993 NameNode
3443 SecondaryNameNode</pre>

		<p>There are a set of log files in the hadooper logs directory. To switch to the directory, type something like:</p>

<pre class="code">cd /usr/local/hadoop/logs/</pre>

<p>Looking through those log files can help you find the errors that may or may not occur.</p>

<p>Here is a command to view a log file:</p>

<pre class="code">cat hadoop-hadooper-datanode-WesternSeas-VirtualBox.log</pre>

<p>There are several different log files in the log directory, you can find them by 
using the<strong> ls</strong> commind to<strong> </strong>look for the files 
that have .log as an extension:</p>
		<pre class="code">$ ls *.log
hadoop-hadooper-datanode-WesternSeas-VirtualBox.log
hadoop-hadooper-jobtracker-WesternSeas-VirtualBox.log
hadoop-hadooper-namenode-WesternSeas-VirtualBox.log
hadoop-hadooper-secondarynamenode-WesternSeas-VirtualBox.log
hadoop-hadooper-tasktracker-WesternSeas-VirtualBox.log</pre>

		<p>And here is the command to stop Hadoop:</p>

		<pre class="code">./stop-all.sh</pre>
		<h2 id="restart">Restart Hadoop</h2>
		<p>If you shut down all the machines in the cluster, it is a good idea 
		to completely restart.</p>
		<p>This script, called <strong>MasterCleanAndRestart.sh</strong> is for 
		the master machine:</p>
		<pre class="code">echo "Stopping Hadoop"
bash /usr/local/hadoop/bin/stop-mapred.sh
bash /usr/local/hadoop/bin/stop-dfs.sh
echo "Refreshing and reformatting file system"
bash CleanAndRestart.sh
echo "Run ClearAndRestart on slave machines"
read -p "Press [Enter] key to re-start hadoop..."
bash /usr/local/hadoop/bin/start-dfs.sh
bash /usr/local/hadoop/bin/start-mapred.sh</pre>
		<p>This script called <strong>CleanAndRestart.sh</strong> is for the 
		client, or slave machines:</p>
		<pre class="code"># Any time you shut down Hadoop altogether, and particularly if you
# shut down the machine it is on, you really ought to clean out the  out the 
# temp files and reformat the drive for the distributed file system, 
# which means you lose all your data.
sudo rm -r /app/hadoop/tmp/
sudo mkdir -p /app/hadoop/tmp
sudo chown -R hadooper:hadoop /app/hadoop/tmp
/usr/local/hadoop/bin/hadoop namenode -format</pre>
		<p>&nbsp;</p>
		<h2 id="multinode">Multinode</h2>
		<p>Begin by starting to Linux VMs on the same machine. In this example, 
		let's pretend that your first VM is named BoxPrimary, and the second VM 
		is named Box02. In giving them these names, we are implicitly deciding 
		that BoxPrimary will be our primary, or master, hadoop server. It will 
		be the job-tracker and the name-node. All the other machines we add to 
		our hadoop network will face toward this machine. You might have 
		assignment your machines different names, but that is an implementation 
		detail. The point is that you have two Linux VMs running, and that you 
		have decided one of them will be the primary hadoop server.</p>
		<p>Edit the /etc/hosts file in both virtual machines so that the two machines can ping one 
		another:</p>
		<ul>
			<li>192.168.56.101&nbsp;&nbsp;&nbsp; BoxPrimary</li>
			<li>192.168.56.102&nbsp;&nbsp;&nbsp; Box02</li>
		</ul>
<p>The actual names and IP addresses you use for the these two boxes will likely 
depend on the names you originally gave to your two Linux VMs, as well as the IP 
addresses assigned to them by the DHCP server. Remember that you can type
<strong>ipconfig</strong> to find out the current IP address of your server. 
Furthermore, you need not use the same name in the hosts file as you gave to 
your VM when you created it, though it might be less confusing if you did indeed 
give them the same name. To test the configuration, go to the command line of
<strong>Box02</strong> and type <strong>ping BoxPrimary. </strong>Then go to the 
command line of BoxPrimary and type <strong>ping Box02</strong>. When you are 
done, press <strong>Ctrl-C </strong>to end the ping session:</p>
		<pre class="code">$ ping Box02
PING Box02 (192.168.56.102) 56(84) bytes of data.
64 bytes from Box02 (192.168.56.123): icmp_req=1 ttl=64 time=2.55 ms
64 bytes from Box02 (192.168.56.123): icmp_req=2 ttl=64 time=0.667 ms</pre>

		<p>Now become Hadooper and copy the SSH public key from the first machine to the second</p>	

<pre class="code">ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadooper@Box02</pre>

<p>When I entered the code shown above, the result looked like this:</p>

<pre class="code">
ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadooper@Box02
The authenticity of host 'Box02 (192.168.0.124)' can't be established.
ECDSA key fingerprint is f9:6e:01:0e:34:d7:3b:6c:3a:bd:78:92:69:21:90:70.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'Box02,192.168.0.124' (ECDSA) to the list of known hosts.
hadooper@Box02's password: 
Now try logging into the machine, with "ssh 'hadooper@westernseas'", and check in:

  ~/.ssh/authorized_keys

to make sure we haven't added extra keys that you weren't expecting.</pre>

<p>Following the hint displayed above, I tried to SSH into Box02:</p>

<pre class="code">
$ ssh hadooper@Box02
Welcome to Ubuntu 11.10 (GNU/Linux 3.0.0-16-generic i686)

 * Documentation:  https://help.ubuntu.com/

0 packages can be updated.
0 updates are security updates.

Last login: Sun Mar  4 13:20:13 2012 from localhost
</pre>

<p>The key thing to notice in the code shown above is that that I was never prompted for a password. That is
the way things should be when an SSH public/private key pair is set up correctly.</p>

		<p>Once you have setup SSH so that you can pop over to Box02 without 
		being prompted for a password, you want to do the same thing in reverse; 
		that is, you want to set things up so that you can ssh from Box02 to 
		BoxPrimary. To begin, log into Box02, and then run the same command you 
		ran in BoxPrimary:</p>
		<pre class="code">ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadooper@BoxPrimary</pre>

		<p>Now check to make sure you can ssh into BoxPrimary without entering a 
		password:</p>
		<pre class="code">ssh hadooper@BoxPrimary</pre>

		<h3>Setup Master and Slave Files</h3>

		<p>You need to edit core-site.xml and madred-site.xml and change the URL 
		from localhost to BoxPrimary in both BoxPrimary and Box02:</p>

		<pre>&lt;property&gt;<br>  &lt;name&gt;fs.default.name&lt;/name&gt;<br>  &lt;value&gt;hdfs://<strong>BoxPrimary</strong>:54310&lt;/value&gt;
&lt;/property&gt;				</pre>

		<p>Make this change also in mapred-site.xml:</p>
		<pre class="code">&lt;property&gt;
&lt;name&gt;mapred.job.tracker&lt;/name&gt;
  &lt;value&gt;BoxPrimary:54311&lt;/value&gt;
&lt;/property&gt;</pre>

		<p>In BoxPrimary, go ahead and start the server</p>
		<pre class="code">./start-dfs.sh</pre>

		<p>In BoxPrimary:</p>
		<pre class="code">$ jps
10581 Jps
10581 Jps
10510 SecondaryNam10100 NameNode</pre>

		<p>In Box02:</p>
		<pre class="code"> jps
6281 J5956 DataNode</pre>

		<p>If you have trouble, check the logs in Box02:</p>
		<pre class="code"> cat hadoop-hadooper-datanode-Box02.log</pre>

		<p>Errors can include something like this:&nbsp; <strong>INFO 
		org.apache.hadoop.ipc.Client: Retrying connect to server: </strong>Errors of that type probably means there is something wrong in the 
		/etc/hosts file either on PrimaryBox or Box02, or perhaps in both 
		places.</p>

		<p>If everything is working, then
		<a href="http://BoxPrimary:50070/dfshealth.jsp">
		http://BoxPrimary:50070/dfshealth.jsp</a> will show at least one live 
		node</p>
		<h3 id="runTest">Running a Test</h3>

		<p>This script is called <strong>RunApp.sh. </strong>It is designed to 
		run one of the example applications that ship with hadoop. It is 
		probably a good idea to completely restart the system between tests, as 
		described above in the <a href="#restart">Restart</a> section.</p>
		<pre class="code">#!/bin/bash

HADOOP="/usr/local/hadoop/bin/hadoop"
echo $HADOOP
DFS=$HADOOP" dfs"
JAR=$HADOOP" jar"
GUTENBERG="/user/hadooper/gutenberg"
OUTPUT=$GUTENBERG"-output"

echo $DFS
$DFS -rmr $GUTENBERG
$DFS -rmr $OUTPUT
$DFS -copyFromLocal $HOME/gutenberg $GUTENBERG
$DFS -ls /user/hadooper
$DFS -ls $GUTENBERG

$JAR /usr/local/hadoop/hadoop-examples-1.0.1.jar wordcount $GUTENBERG $OUTPUT
read -p "Press [Enter] key to see the results"
/usr/local/hadoop/bin/hadoop dfs -cat /user/hadooper/gutenberg-o/usr/local/hadoop/bin/hadoop dfs -cat /user/hadooper/gutenberg-output/part-r-00000</pre>
		<h2 id="scripts">The Scripts</h2>
		<p>If you download the <a href="Mercurial.html">mercurial sources</a> 
		linked to Elvenware you will find a number of the scripts shown on this 
		page in the Python/CreateHadoopFiles directory. Here is how to use them.</p>

		<p>When you download the files from elvenware repository, it is often 
		helpful to copy the hadoop files into a folder called bin:</p>
		<pre class="code">/home/hadooper/bin</pre>

		<p>If you are already in <strong>bin</strong>, then you can use this 
		command to copy the files from the downloaded repsoitory into your 
		current directory, so long as you have the most recent version of the 
		repository in <strong>/home/hadooper/andelf</strong>:</p>
		<pre class="code">cp /home/hadooper/andelf/Python/CreateHadoopFiles/src/* .</pre>

		<p>Then you need to run one or more of the scripts. To prepare them, 
		first make them executable:</p>
		<pre class="code">chmod +x *.sh</pre>

		<p>You can now run the scripts by typing ./SomeScript.sh. For instance, 
		you might do this to execute the script called GetBooks:</p>
		<pre class="code">./GetBooks.sh</pre>

		<p>After running the script, you should find that James Joyce's <strong>
		Ulysses</strong> and other books have been downloaded and stored in the 
		following folder:</p>
		<pre class="code">/home/hadooper/gutenberg</pre>

		<p>Considered as a whole, the Hadooper scripts from the repository are 
		designed to to perform certain key steps, such as downloading and 
		installing Hadoop, or setting up SSH, or running the application.</p>
		<p>Here is a good order for running the scripts:</p>
		<ul>
			<li>Get <a href="#java">Java</a> set up one way or another. The 
			script <strong>./addSun.sh 			</strong>will do this for you, but <strong>cat</strong>  it out first 
			so you understand what it does.</li>
			<li>Create the user by running <strong>./CreateUsers.sh</strong>.</li>
		</ul>
		<p>After creating the users will automatically be running as the user 
		Hadoop. Now:</p>
		<ul>
			<li>Run the ssh script: <strong>./setupSsh.sh</strong></li>
			<li>Run <strong>addNano.sh</strong> which installs hadoop. (Sorry 
			about the naming, I need to fix it change the name to <strong>
			addHadoop.sh</strong>. At this stage, you may need to become the 
			regular user for a moment to be sure that the newly created <strong>
			.bashrc</strong> 
			gets run.</li>
			<li>Run the python script called <strong>CreateScripts.sh: python 
			CreateScripts.py</strong></li>
		</ul>
		
		<p>There is some manual configuration you will need to do with the host 
		files and copying the .ssh keys back to the primary or master box. After 
		that, you are essentially ready to run. I would go through this process 
		after you bring up all the nodes, and before you first run an 
		application:</p>
		<ul>
			<li>On the Master machine, run <strong>./MasterCleanAndRestart.sh</strong></li>
			<li>The script will pause halfway through. During the pause, run 
			./CleanAndRestart.sh on each of the clients.</li>
			<li>Come back and press enter to finish running <strong>
			./MasterCleanAndRestart.sh</strong></li>
			<li>Now you can run the test: <strong>./RunApp.sh</strong></li>
		</ul>
		
		<p>As long as you don't shut down the machines, you shouldn't have to bother with 
		running <strong>./MasterCleanAndRestart.sh</strong> or<strong> 
		./CleanAndRestart.sh</strong> inbetween test runs The problems I've had 
		seem to occur when the file systems get out of sync on the various data 
		nodes. Once that happens, I think it is simplest to start over, but 
		below in the Linkx section you can see a note about namespaceids.</p>

		<p>The other problem:: I have had the whole process of running the 
		application stop during the Reduce phase at some point such as 22%. 
		There is a very long pause, maybe five minutes, and then you get an 
		error about too many something or others being created, and then the 
		process finishes normally. When I encountered this error, it meant that 
		I had one or or more of my hosts files incorrectly configured, by which 
		I mean I simply had a type in the name of one of the hosts, or had left 
		out a host, or had typed in an IP address incorrectly. Nothing tricky, 
		but just wrong.</p>
	

<h2 id="links">Links</h2>
<ul>
	<li class="auto-style1">
	<a href="http://wiki.apache.org/hadoop/GettingStartedWithHadoop">
	http://wiki.apache.org/hadoop/GettingStartedWithHadoop</a></li>
	<li>
	<a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/" target="_blank">http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/</a>
	</li>
	<li>
	<a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/#java-io-ioexception-incompatible-namespaceids">
	java-io-ioexception-incompatible-namespaceids</a></li>
</ul>

<p>&nbsp;</p>



		<!-- #EndEditable -->
	</article>
	<footer>

		<p>Copyright &copy; <a href="../../index.html">Charlie Calvert</a> |
		<a href="../../index.html">Elvenware Home</a> |
		<a href="../index.html">Writing Code</a> |
		<a href="../delphi/index.html">Delphi</a> |
		<a href="../csharp/index.html">CSharp</a> |
		<a href="../../books/index.html">My Books</a> </p>
	</footer>
</div>

</body>

<!-- #EndTemplate -->

</html>
