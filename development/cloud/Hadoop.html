<!DOCTYPE html><html lang="en" dir="ltr"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width initial-scale=1.0"><title>Hadoop</title><link href="/css/first-style.css" rel="stylesheet"><link rel="shortcut icon" href="/images/favicon.png"><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet"><link href="/css/highlight/googlecode.css" rel="stylesheet" type="text/css"><link href="/css/style.css" rel="stylesheet"><script src="https://code.jquery.com/jquery-1.12.0.min.js"></script><script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<!-- Latest compiled and minified CSS --><script src="/js/elven-help.js"></script></head><body><nav class="navbar navbar-inverse"><div class="container"><div class="navbar-header"><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand" href="/">Elvenware</a></div><div class="navbar-collapse collapse" id="navbar"><ul class="nav navbar-nav"><li class="trigger-collapse" ng-class="{ active: isActive('/')}"><a href="/">Home</a></li><li><a href="/about.html">About</a></li></ul></div></div></nav><div class="container"><figure><img class="elf-normal" alt="Elvenware" src="/images/elvenwarelogo.png"/></figure><h1>Hadoop</h1><p>Welcome to Hadoop</p><ul><!--TOC_Start--><li><a href="#charlie-calvert-on-elvenware">Charlie Calvert on Elvenware</a></li>
<li><a href="#writing-code-and-prose-on-computers">Writing Code and Prose on Computers</a></li>
<li><a href="#menu">Menu</a></li>
<li><a href="#core-code">Core Code</a></li>
<li><a href="#os-and-tools">OS and Tools</a></li>
<li><a href="#art">Art</a></li>
<li><a href="#links">Links</a></li>
<li><a href="#index">Index</a></li>
<li><a href="#what">What is Hadoop</a></li>
<li><a href="#mapReduce">MapReduce and HDFS</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#mercurial">Mercurial</a></li>
<li><a href="#java">Install Java</a></li>
<li><a href="#user">User Setup</a></li>
<li><a href="#ssh">SSH Setup</a></li>
<li><a href="#hadoop">Install Hadoop</a></li>
<li><a href="#environment">Set up the Environment</a></li>
<li><a href="#configure">Configure Hadoop</a></li>
<li><a href="#startHadoop">Starting Hadoop</a></li>
<li><a href="#restart">Restart Hadoop</a></li>
<li><a href="#multinode">Multinode</a></li>
<li><a href="#setup-master-and-slave-files">Setup Master and Slave Files</a></li>
<li><a href="#runTest">Running a Test</a></li>
<li><a href="#scripts">The Scripts</a></li><!--TOC_End--></ul></div><div class="container"><p>Toggle Menu</p>
<a class="anchor" id="charlie-calvert-on-elvenware"></a>
<h1>Charlie Calvert on Elvenware</h1>
<a class="anchor" id="writing-code-and-prose-on-computers"></a>
<h2>Writing Code and Prose on Computers</h2>
<a class="anchor" id="menu"></a>
<h2>Menu</h2>
<a class="anchor" id="core-code"></a>
<h2>Core Code</h2>
<ul>
<li><a href="../index.html">Strongly Typed</a></li>
<li><a href="../web/index.html">Web &amp; Scripts</a></li>
<li><a href="index.shtml">Cloud</a></li>
</ul>
<a class="anchor" id="os-and-tools"></a>
<h2>OS and Tools</h2>
<ul>
<li><a href="../../os/index.html">OS</a></li>
<li><a href="../database/index.html">Database</a></li>
<li><a href="../../books/index.html">My Writing</a></li>
</ul>
<a class="anchor" id="art"></a>
<h2>Art</h2>
<ul>
<li><a href="../../Art/index.html">Poems &amp; Photos</a></li>
<li><a href="../../books/reading/index.html">Book Reviews</a></li>
<li><a href="../../spirit/index.html">Spiritual</a></li>
</ul>
<a class="anchor" id="links"></a>
<h2>Links</h2>
<ul>
<li><a href="../../links.html">My Links</a></li>
<li><a href="http://www.falafel.com/">Falafel</a></li>
<li><a href="http://sourceforge.net/projects/elvenware/">Sourceforge</a></li>
</ul>
<p><img src="../../images/elvenwarelogo.png" alt="Elvenware"></p>
<a class="anchor" id="index"></a>
<h2>Index</h2>
<ul>
<li><a href="#what">What is Hadoop</a></li>
<li><a href="#mapReduce">MapReduce and HDFS</a></li>
<li><a href="#mercurial">The Elvenware Mercurial Repository Hadoop Setup
Scripts</a></li>
<li>Install <a href="#java">Java</a></li>
<li><a href="#user">Set up a User Called Hadooper</a></li>
<li>Install and setup <a href="#ssh">SSH</a></li>
<li>Install <a href="#hadoop">Hadoop</a></li>
<li><a href="#environment">Set up the Environment</a></li>
<li><a href="#configure">Configure Hadoop</a></li>
<li><a href="#startHadoop">Start Hadoop</a></li>
<li><a href="#restart">Restart Hadoop</a></li>
<li><a href="#multinode">Multinode</a></li>
<li><a href="#runTest">Running a Test Application</a></li>
<li><a href="#scripts">Understanding the Scripts</a></li>
<li><a href="#links">Links</a></li>
</ul>
<a class="anchor" id="what"></a>
<h2>What is Hadoop</h2>
<p>A typical hard drive is 2 terabytes (2 TB) in size. It takes about five
hours to read all the data on a single disk. If we could split the data
over 200 hard drives, all 2 TB could be read in about 5 minutes. This is
possible because each drive contains only 1/200th of the data, and thus
needs to read much less data to finish its part of the task. It is the
same principle that is involved with asking 1 person with a single
gallon bucket to move a 2000 gallons of water vs. asking 200 people with
200 one gallon buckets to move 2000 gallons of water. If a trip takes
one minute, then one person can do the job in 2000 minutes or 33 hours.
200 people, on the other hand, can do the job in 10 minutes. This is the
power of distributed, parallel computing, and it is the task undertaken
by Hadoop.</p>
<p>One of the goals of Hadoop is to make it possible to do distributed data
over multiple drives bought at a typical computer store. We are not
talking about some huge mainframe, but about off the shelf drives you
might buy at Amazon, Newegg or at Fry&#39;s Electronics.</p>
<p>Hadoop is designed to solve problems associated with huge datasets. If
you a site gets 100 million visitors each month, each of which leaves a
trail of data one megabyte in size, then the site must track 100 million
megabytes a month, or over a billion megabytes a year.</p>
<p>Hadoop was partially developed at Google, and they use it to solve many
of their problems.</p>
<a class="anchor" id="mapReduce"></a>
<h2>MapReduce and HDFS</h2>
<p>If we are going to distribute data over 100 hard drives, then we need to
find a way to reassemble it, or to combine it with other bits of data
quickly and easily. The part of Hadoop called MapReduce addresses this
issue. Map/Reduce assembles the bits of data across the various hard
drives, but it does more than that. It allows developers to create
applications that run concurrently across all the nodes of the system,
with one set of tasks performed on one machine, another on a second
machine and so on. That is the Map part of the equation. Then the reduce
part of the equation takes all these distributed pieces of work and
assembles the results.</p>
<p>Suppose ten million visitors</p>
<p>The part of Hadoop called HDFS deals with the problem of hardware
failure. If we are going to spread out data over 100 hard drives, then
the chance of loosing some of it due to hardware failure is higher than
it might be on a single machine. The goal is to duplicate the data, so a
loss of one hard drive will not destroy the entire terabyte of data.</p>
<a class="anchor" id="setup"></a>
<h2>Setup</h2>
<p>Most of the rest of this document describes one possible way to set up
Hadoop with a minimal configuration for testing Hadoop on a single
machine. Hadoop normally runs in multi-user mode, but here we are going
to use only a single machine. This kind of setup provides a good way for
you to learn the basics of hadoop before you begin deployment across
multiple machines.</p>
<a class="anchor" id="mercurial"></a>
<h3>Mercurial</h3>
<p>I have created a set of scripts that can help step you through the
Hadoop install. They are stored with a number of other files and
programs in the Elvenware repository. Instructions on setting up
Mercurial are now on the Mercurial page:</p>
<pre><code class="lang-%7B.code%7D">http:<span class="hljs-regexp">//</span>www.elvenware.com<span class="hljs-regexp">/charlie/</span>development<span class="hljs-regexp">/cloud/</span>Mercurial.html
</code></pre>
<a class="anchor" id="java"></a>
<h3>Install Java</h3>
<p>Setting up Java can be a bit of a problem. It is best to get it out of
the way first. I have tested with both Sun JDK6 and Sun JDK7, and both
seem to work. Below I focus mosting on JDK6, but here is a post on
<a href="http://askubuntu.com/questions/55848/how-do-i-install-oracle-java-jdk-7">installing
JDK7</a>.</p>
<p>NOTE: I have recently come to rely on the simple method found here:</p>
<pre><code>http://www.webupd8.org/2012/01/<span class="hljs-keyword">install</span>-<span class="hljs-keyword">oracle</span>-<span class="hljs-keyword">java</span>-jdk<span class="hljs-number">-7</span>-<span class="hljs-keyword">in</span>-ubuntu-via.html
</code></pre><p>By default, you do not have the right version of Java on Ubuntu Linux.
The following commands should fix the situation:</p>
<pre><code class="lang-%7B.code%7D">wget http<span class="hljs-variable">s:</span>//raw.github.<span class="hljs-keyword">com</span>/flexiondotorg/oab-java6/master/oab-java6.<span class="hljs-keyword">sh</span> -O oab-java6.<span class="hljs-keyword">sh</span>
chmod +<span class="hljs-keyword">x</span> oab-java6.<span class="hljs-keyword">sh</span>
sudo ./oab-java6.<span class="hljs-keyword">sh</span>
</code></pre>
<p>After the endless process outlined above finally terminates, do this:</p>
<pre><code class="lang-%7B.code%7D">sudo apt-get <span class="hljs-keyword">install </span>sun-<span class="hljs-keyword">java6-jdk
</span>sudo update-alternatives --<span class="hljs-built_in">config</span> <span class="hljs-keyword">java</span>
</code></pre>
<p>More information on the scripts shown above is available here:</p>
<p><a href="https://github.com/flexiondotorg/oab-java6">https://github.com/flexiondotorg/oab-java6</a></p>
<p>This should work to setup Java Sun on Mint Linux:</p>
<pre><code class="lang-%7B.code%7D">sudo<span class="hljs-built_in"> add-apt-repository </span><span class="hljs-string">"deb http://archive.canonical.com/ lucid partner"</span>
sudo apt-get update
sudo apt-get install sun-java6-jdk
sudo update-java-alternatives -s java-6-sun
</code></pre>
<p>On one of my system, a day or more after completing  the above steps,
when I ran <strong>sudo apt-get update</strong> I got the following error:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-attribute">W</span>: GPG <span class="hljs-attribute">error</span>: <span class="hljs-attribute">http</span>:<span class="hljs-comment">//ppa.launchpad.net oneiric Release: The following signatures </span>
couldn't be verified because the public key is not <span class="hljs-attribute">available</span>: NO_PUBKEY <span class="hljs-number">2</span>EA8F35793D8809A
</code></pre>
<p>To elimate the error, I used the menu to go to <strong>System Settings |
Software Sources | Other Software</strong> and unchecked<strong>** the references to
flexiondotorg. Then I could run </strong>sudo apt-get update** without error.</p>
<a class="anchor" id="user"></a>
<h3>User Setup</h3>
<p>It is optional as to whether or not to create a user called Hadoop, but
I strongly recommend that you do so. This may have security benefits,
but we like it primarily because it keeps all the Hadoop related
configuration in one place. In the process a custom .<strong>bashrc</strong> file is
set up just for the Hadoop user, which means that we can configure the
environment just for him withou muddying up the configuration your
primary user account, which can be nice.</p>
<p>This code create a new group called hadoop, adds a user to it, adds the
user to the admins group and then switches you over from being yourself
to being hadooper. There will be a number of prompts you need to respond
to on the way.</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-symbol">sudo</span> <span class="hljs-keyword">addgroup </span>hadoop
<span class="hljs-symbol">sudo</span> <span class="hljs-keyword">adduser </span>--ingroup hadoop hadooper
<span class="hljs-symbol">sudo</span> usermod -a -G admin hadooper
<span class="hljs-symbol">su</span> -l hadooper
</code></pre>
<p>I have created a script called <strong>CreateUser.sh</strong> that performs these
actions automatically. See the <a href="#scripts">end of this document</a>for more
details.\</p>
<a class="anchor" id="ssh"></a>
<h3>SSH Setup</h3>
<p>For Hadoop to work, you have to be able to use SSH to communicate
between two machines. In some cases, you will even need to be able to
SSH into your current machine. There is generally no real world use for
using SSH to communicate with your own machine, but sometimes it can be
part of a testing process. In particular, if you are setting up HADOOP
to run on a single node for testing or educational purposes, then you
will need to SSH into your current machine. I cover these aspects of SSH
in a separate file:</p>
<ul>
<li><a href="SshFtpsPutty.html#sshKeys">Documentation on SSH and the <strong>authorized_keys</strong>
file.</a></li>
</ul>
<p>Read the appropriate documentation on SSH and then return to this
document after you have learned how to use SSH to start an SSH session
on your localhost.</p>
<a class="anchor" id="hadoop"></a>
<h2>Install Hadoop</h2>
<p>Assuming you have completed the above steps, you are now ready to
install Hadoop. This can be accomplished by running the <strong>addNano.sh</strong>
script discussed at <a href="#scripts">the end of this document</a>. What follows
is a description of that script does.</p>
<p>Here is the download page for Hadoop:</p>
<pre><code class="lang-%7B.code%7D">http:<span class="hljs-regexp">//</span>www.apache.org<span class="hljs-regexp">/dyn/</span>closer.cgi<span class="hljs-regexp">/hadoop/</span>common<span class="hljs-regexp">/</span>
</code></pre>
<p>You can also try to download Hadoop by issuing the following command at
the Linux command prompt:</p>
<pre><code class="lang-%7B.code%7D">wget http:<span class="hljs-regexp">//</span>apache.cs.utah.edu<span class="hljs-regexp">//</span>hadoop<span class="hljs-regexp">/common/</span>hadoop-<span class="hljs-number">1.0</span>.<span class="hljs-number">1</span><span class="hljs-regexp">/hadoop-1.0.1.tar.gz</span>
</code></pre>
<p>The following script downloads and extracts hadoop:</p>
<pre><code class="lang-%7B.code%7D">wget <span class="hljs-string">http:</span><span class="hljs-comment">//apache.cs.utah.edu//hadoop/common/hadoop-1.0.1/hadoop-1.0.1.tar.gz</span>
tar xzf hadoop<span class="hljs-number">-1.0</span><span class="hljs-number">.1</span>
sudo mv hadoop<span class="hljs-number">-1.0</span><span class="hljs-number">.1</span>.tar.gz <span class="hljs-regexp">/usr/</span>local/hadoop
sudo chown -R <span class="hljs-string">hadooper:</span>hadoop <span class="hljs-regexp">/usr/</span>local/hadoop
</code></pre>
<p> The scripts starts by downloading a file that is both zipped (gz) and
tarred (.tar):</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-selector-tag">hadoop-1</span><span class="hljs-selector-class">.0</span><span class="hljs-selector-class">.1-bin</span><span class="hljs-selector-class">.tar</span><span class="hljs-selector-class">.gz</span>
</code></pre>
<p>Take a moment to examine this file name, and particular the part at the
end. The <strong>tar</strong> extensions means that many files have been wrapped
together in one big file called <strong>hadoop-1.0.1-bin.tar</strong>. Then gzip
compressess that file into a file called <strong>hadoop-1.0.1.tar.gz</strong>. The
following command reverses the process by unzipping the tar file and
then extracting (untarring) the contents:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-selector-tag">tar</span> <span class="hljs-selector-tag">xzf</span> <span class="hljs-selector-tag">hadoop-1</span><span class="hljs-selector-class">.0</span><span class="hljs-selector-class">.1-bin</span><span class="hljs-selector-class">.tar</span><span class="hljs-selector-class">.gz</span> 
</code></pre>
<p>After the command is run you should see a folder called hadoop-1.0.1.
You can usually tell a folder from a file because it is shown in light
blue, and because its permissions begin with drwxr-etc.</p>
<p>The script then moves (mv) the folder to /usr/local/hadoop. The command
does two things: it moves the folder to a new location, and then renames
it by removing the version number. When you are done, you should be able
to see the contents of the folder:</p>
<pre><code class="lang-%7B.code%7D">charlie@MintBox ~/Downloads $ ls /usr/local/hadoop/
bin hadoop-client-<span class="hljs-number">1.0</span>.<span class="hljs-number">1</span><span class="hljs-selector-class">.jar</span> ivy<span class="hljs-selector-class">.xml</span> sbin
build<span class="hljs-selector-class">.xml</span> hadoop-core-<span class="hljs-number">1.0</span>.<span class="hljs-number">1</span><span class="hljs-selector-class">.jar</span> lib share
c++ hadoop-examples-<span class="hljs-number">1.0</span>.<span class="hljs-number">1</span><span class="hljs-selector-class">.jar</span> libexec src
CHANGES<span class="hljs-selector-class">.txt</span> hadoop-minicluster-<span class="hljs-number">1.0</span>.<span class="hljs-number">1</span><span class="hljs-selector-class">.jar</span> LICENSE<span class="hljs-selector-class">.txt</span> test<span class="hljs-selector-class">.sh</span>
conf hadoop-test-<span class="hljs-number">1.0</span>.<span class="hljs-number">1</span><span class="hljs-selector-class">.jar</span> logs webapps
contrib hadoop-tools-<span class="hljs-number">1.0</span>.<span class="hljs-number">1</span><span class="hljs-selector-class">.jar</span> NOTICE<span class="hljs-selector-class">.txt</span>
hadoop-ant-<span class="hljs-number">1.0</span>.<span class="hljs-number">1</span><span class="hljs-selector-class">.jar</span> ivy README<span class="hljs-selector-class">.txt</span>
charlie@MintBox ~/Downloads $ 
</code></pre>
<a class="anchor" id="environment"></a>
<h2>Set up the Environment</h2>
<p>It is now time to set up the environment. There are two useful, but
optional, environment variables that we can set up, plus we must set up
JAVA_HOME. To set up these ennvironment variables, we could type the
following each time we become Hadooper:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/usr/lib/jvm/default-java
</code></pre>
<p>The above is preferred, but alternatively, you can explicitly name the
version you want to use:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/usr/lib/jvm/java-6-sun
</code></pre>
<p>Rather than trying to configure these items by hand each time we become
Hadooper, it is better to put them in a file called <strong>.bashrc</strong>. To
oversimplify a somewhat complex subject, I&#39;ll say only that the
<strong>.bashrc</strong> file allows us to configure the environment automatically
each time we log in to the bash shell. This occurs because <strong>.bashrc</strong>
is run once, just as we are signing in as hadooper. You will recall that
to sign in as hadooper, we usually write something like this: <strong>su -
hadooper</strong>.</p>
<p>NOTE: <em>Files that begin with a period are &quot;invisible&quot; or &quot;hidden&quot; by
default. If we type</em> <strong>*ls**</strong>to get a listing of a directory, we don&#39;t
see them. To make them &quot;visible,&quot; we should type* <strong>ls -</strong>a:</p>
<pre><code class="lang-%7B.code%7D">$ ls
andelf bar Downloads examples<span class="hljs-selector-class">.desktop</span>
$ ls -<span class="hljs-selector-tag">a</span>
. bar <span class="hljs-selector-class">.bashrc</span> examples<span class="hljs-selector-class">.desktop</span> <span class="hljs-selector-class">.sudo_as_admin_successful</span> .. 
<span class="hljs-selector-class">.bash_history</span> <span class="hljs-selector-class">.cache</span> <span class="hljs-selector-class">.profile</span> andelf <span class="hljs-selector-class">.bash_logout</span> Downloads .ssh
</code></pre>
<p>Here is the code we want to put in the .<strong>bashrc</strong> file:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-comment"># Set JAVA_HOME:</span>
<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/usr/lib/jvm/default-java
</code></pre>
<p>The first line is a comment, the second sets a variable in the
environment of our OS. Our goal, then, is to create a file called
.<strong>bashrc</strong> in our HOME directory, which will be <strong>/home/hadooper</strong>.
Type <strong>cd</strong> followed by enter with no parameters to move to your home
directory, and then to check which directory you are in right now, type
<strong>pwd</strong>:</p>
<pre><code class="lang-%7B.code%7D">hadooper<span class="hljs-variable">@WesternSeas</span><span class="hljs-symbol">:~</span><span class="hljs-variable">$/</span>bin cd
hadooper<span class="hljs-variable">@WesternSeas</span><span class="hljs-symbol">:~</span><span class="hljs-variable">$ </span>pwd
/home/hadooper
</code></pre>
<p>As you can see, we are now in <strong>hadooper&#39;s</strong> home directory, which is
<strong>/home/hadooper.</strong>That&#39;s just where we want to be.</p>
<p>The home directory is where you want to create your .<strong>bashrc</strong> file. To
create it, type:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-selector-tag">nano</span> <span class="hljs-selector-class">.bashrc</span>
</code></pre>
<p>You may find that .<strong>bashrc</strong> already exists, and already has the right
entries in it, as one of my scripts takes care of that for you. But if
you want to do it all by hand, then you should enter the following code
into .<strong>bashrc</strong>, then type <strong>Ctrl-O</strong> plus <strong>enter</strong> to save and
<strong>Ctrl-X</strong> to exit:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-comment"># Set JAVA_HOME:</span>
<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/usr/lib/jvm/default-java
</code></pre>
<p>At this stage we need to run the .bashrc file so that the environment
will be properly set up. The best way to run the file is to temporarilly
stop being hadooper, and then sign back in again. To do this, type exit
once or twice, until you become the regular user again, that is, until
you exit the <strong>hadooper</strong> shell. Then log back in as hadooper:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-attribute">su - hadooper</span>
</code></pre>
<p>When you log back in your .<strong>bashrc</strong> file will run automatically, as
indeed it will each time you log in from this point forward. That&#39;s the
whole point of .<strong>bashrc</strong>, it is a file that is run when you log into
the bash shell; you place in that file any code that you want to run at
the beginning of a user session. The type of code we placed in
.<strong>bashrc</strong> is typical, in that it is designed to set up the
environment. In particular, when your .<strong>bashrc</strong> file runs,
<strong>JAVA_HOME</strong> variables in the environment. To check this, type echo
\$<strong>JAVA_HOME</strong>, etc:</p>
<pre><code class="lang-%7B.code%7D">echo <span class="hljs-variable">$JAVA_HOME</span>
<span class="hljs-regexp">/usr/</span>lib<span class="hljs-regexp">/jvm/</span>java-<span class="hljs-number">6</span>-sun
echo <span class="hljs-variable">$PATH</span>
<span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/bin:/u</span>sr<span class="hljs-regexp">/bin:/</span>bin:<span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/games:/u</span>sr<span class="hljs-regexp">/games:/u</span>sr<span class="hljs-regexp">/local/</span>hadoop<span class="hljs-regexp">/bin</span>
</code></pre>
<p>As you can see, all the environment variables that we wanted to
configure are now set up correctly.</p>
<p>Alternatively, the following can be used to test the status of an
environment variable:</p>
<pre><code class="lang-%7B.code%7D">%7B%24JAVA_HOME%3A%3F%7D
</code></pre>
<p>We are ready to move on to the next step.\</p>
<a class="anchor" id="configure"></a>
<h2>Configure Hadoop</h2>
<p>There are several files found in the <strong>conf</strong>directory that you need to
configure:</p>
<ul>
<li><strong>Common properties</strong>: core-site.xml</li>
<li><strong>Configure HDFS</strong>: hdfs-site.xml</li>
<li><strong>Configure MapReduce</strong>: mapred-site.xml</li>
</ul>
<p>And here is what goes in each file:</p>
<pre><code><span class="php"><span class="hljs-meta">&lt;?</span>xml version=<span class="hljs-string">"1.0"</span><span class="hljs-meta">?&gt;</span></span> 
<span class="hljs-comment">&lt;!-- core-site.xml --&gt;</span> 
<span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span> 
  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> 
    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.default.name<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> 
    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://localhost/<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span> 
  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span> 
<span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span> 

<span class="php"><span class="hljs-meta">&lt;?</span>xml version=<span class="hljs-string">"1.0"</span><span class="hljs-meta">?&gt;</span></span> 
<span class="hljs-comment">&lt;!-- hdfs-site.xml --&gt;</span> 
<span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span> 
  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> 
    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> 
    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span> 
  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span> 
<span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span> 

<span class="php"><span class="hljs-meta">&lt;?</span>xml version=<span class="hljs-string">"1.0"</span><span class="hljs-meta">?&gt;</span></span> 
<span class="hljs-comment">&lt;!-- mapred-site.xml --&gt;</span> 
<span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span> 
  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> 
    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapred.job.tracker<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> 
    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>localhost:8021<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span> 
  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span>
</code></pre><p>Also be sure you set up the tmp directory and format the namenode:</p>
<pre><code class="lang-%7B.code%7D">sudo mkdir -p <span class="hljs-regexp">/app/</span>hadoop/tmp
sudo chown -R <span class="hljs-string">hadooper:</span>hadoop <span class="hljs-regexp">/app/</span>hadoop/tmp
<span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/hadoop/</span>bin/hadoop namenode -format
</code></pre>
<a class="anchor" id="startHadoop"></a>
<h2>Starting Hadoop</h2>
<p><strong><em>This section under construction....</em></strong></p>
<p>To start Hadoop, run the scripts in the bin directory:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/hadoop/</span>bin
</code></pre>
<p>Assuming you have navigated to the bin directory, the command to start
Hadoop looks liks this:</p>
<pre><code class="lang-%7B.code%7D">./start-all<span class="hljs-selector-class">.sh</span> 
</code></pre>
<p>To see if Hadoop started correctly, use the jps command. The output
should look something like this:</p>
<pre><code class="lang-%7B.code%7D">$ jps
<span class="hljs-number">3216</span> DataNode
<span class="hljs-number">3524</span> JobTracker
<span class="hljs-number">3917</span> Jps
<span class="hljs-number">3759</span> TaskTracker
<span class="hljs-number">2993</span> NameNode
<span class="hljs-number">3443</span> SecondaryNameNode
</code></pre>
<p>There are a set of log files in the hadooper logs directory. To switch
to the directory, type something like:</p>
<pre><code class="lang-%7B.code%7D">cd <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/hadoop/</span>logs<span class="hljs-regexp">/</span>
</code></pre>
<p>Looking through those log files can help you find the errors that may or
may not occur.</p>
<p>Here is a command to view a log file:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-built_in">cat</span> hadoop-hadooper-datanode-WesternSeas-VirtualBox.<span class="hljs-built_in">log</span>
</code></pre>
<p>There are several different log files in the log directory, you can find
them by using the<strong>ls</strong> commind to<em>**</em>look for the files that have .log
as an extension:</p>
<pre><code class="lang-%7B.code%7D">$ ls *<span class="hljs-selector-class">.log</span>
hadoop-hadooper-datanode-WesternSeas-VirtualBox<span class="hljs-selector-class">.log</span>
hadoop-hadooper-jobtracker-WesternSeas-VirtualBox<span class="hljs-selector-class">.log</span>
hadoop-hadooper-namenode-WesternSeas-VirtualBox<span class="hljs-selector-class">.log</span>
hadoop-hadooper-secondarynamenode-WesternSeas-VirtualBox<span class="hljs-selector-class">.log</span>
hadoop-hadooper-tasktracker-WesternSeas-VirtualBox.log
</code></pre>
<p>And here is the command to stop Hadoop:</p>
<pre><code class="lang-%7B.code%7D">./<span class="hljs-keyword">stop</span>-<span class="hljs-keyword">all</span>.<span class="hljs-keyword">sh</span>
</code></pre>
<a class="anchor" id="restart"></a>
<h2>Restart Hadoop</h2>
<p>If you shut down all the machines in the cluster, it is a good idea to
completely restart.</p>
<p>This script, called <strong>MasterCleanAndRestart.sh</strong> is for the master
machine:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-keyword">echo</span> <span class="hljs-string">"Stopping Hadoop"</span>
bash /usr/local/hadoop/bin/<span class="hljs-keyword">stop</span>-mapred.<span class="hljs-keyword">sh</span>
bash /usr/local/hadoop/bin/<span class="hljs-keyword">stop</span>-dfs.<span class="hljs-keyword">sh</span>
<span class="hljs-keyword">echo</span> <span class="hljs-string">"Refreshing and reformatting file system"</span>
bash CleanAndRestart.<span class="hljs-keyword">sh</span>
<span class="hljs-keyword">echo</span> <span class="hljs-string">"Run ClearAndRestart on slave machines"</span>
<span class="hljs-keyword">read</span> -<span class="hljs-keyword">p</span> <span class="hljs-string">"Press [Enter] key to re-start hadoop..."</span>
bash /usr/local/hadoop/bin/start-dfs.<span class="hljs-keyword">sh</span>
bash /usr/local/hadoop/bin/start-mapred.<span class="hljs-keyword">sh</span>
</code></pre>
<p>This script called <strong>CleanAndRestart.sh</strong> is for the client, or slave
machines:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-comment"># Any time you shut down Hadoop altogether, and particularly if you</span>
<span class="hljs-comment"># shut down the machine it is on, you really ought to clean out the  out the </span>
<span class="hljs-comment"># temp files and reformat the drive for the distributed file system, </span>
<span class="hljs-comment"># which means you lose all your data.</span>
sudo rm -r <span class="hljs-string">/app/hadoop/tmp/</span>
sudo mkdir -p <span class="hljs-string">/app/hadoop/tmp</span>
sudo chown -R hadooper<span class="hljs-function">:hadoop</span> <span class="hljs-string">/app/hadoop/tmp</span>
<span class="hljs-string">/usr/local/hadoop/bin/hadoop</span> namenode -format
</code></pre>
<a class="anchor" id="multinode"></a>
<h2>Multinode</h2>
<p>Begin by starting to Linux VMs on the same machine. In this example,
let&#39;s pretend that your first VM is named BoxPrimary, and the second VM
is named Box02. In giving them these names, we are implicitly deciding
that BoxPrimary will be our primary, or master, hadoop server. It will
be the job-tracker and the name-node. All the other machines we add to
our hadoop network will face toward this machine. You might have
assignment your machines different names, but that is an implementation
detail. The point is that you have two Linux VMs running, and that you
have decided one of them will be the primary hadoop server.</p>
<p>Edit the /etc/hosts file in both virtual machines so that the two
machines can ping one another:</p>
<ul>
<li>192.168.56.101    BoxPrimary</li>
<li>192.168.56.102    Box02</li>
</ul>
<p>The actual names and IP addresses you use for the these two boxes will
likely depend on the names you originally gave to your two Linux VMs, as
well as the IP addresses assigned to them by the DHCP server. Remember
that you can type <strong>ipconfig</strong> to find out the current IP address of
your server. Furthermore, you need not use the same name in the hosts
file as you gave to your VM when you created it, though it might be less
confusing if you did indeed give them the same name. To test the
configuration, go to the command line of <strong>Box02</strong> and type <strong>ping
BoxPrimary.</strong>Then go to the command line of BoxPrimary and type <strong>ping
Box02</strong>. When you are done, press <strong>Ctrl-C</strong>to end the ping session:</p>
<pre><code class="lang-%7B.code%7D">$<span class="hljs-built_in"> ping </span>Box02<span class="hljs-built_in">
PING </span>Box02 (192.168.56.102) 56(84) bytes of data.
64 bytes <span class="hljs-keyword">from</span> Box02 (192.168.56.123): <span class="hljs-attribute">icmp_req</span>=1 <span class="hljs-attribute">ttl</span>=64 <span class="hljs-attribute">time</span>=2.55 ms
64 bytes <span class="hljs-keyword">from</span> Box02 (192.168.56.123): <span class="hljs-attribute">icmp_req</span>=2 <span class="hljs-attribute">ttl</span>=64 <span class="hljs-attribute">time</span>=0.667 ms
</code></pre>
<p>Now become Hadooper and copy the SSH public key from the first machine
to the second</p>
<pre><code class="lang-%7B.code%7D">ssh-copy-id -<span class="hljs-selector-tag">i</span> <span class="hljs-variable">$HOME</span>/.ssh/id_rsa<span class="hljs-selector-class">.pub</span> hadooper@Box02
</code></pre>
<p>When I entered the code shown above, the result looked like this:</p>
<pre><code class="lang-%7B.code%7D">ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadooper@Box02
The authenticity <span class="hljs-keyword">of</span> host <span class="hljs-comment">'Box02 (192.168.0.124)' can't be established.</span>
ECDSA <span class="hljs-keyword">key</span> fingerprint <span class="hljs-keyword">is</span> f9:<span class="hljs-number">6</span>e:<span class="hljs-number">01</span>:<span class="hljs-number">0</span>e:<span class="hljs-number">34</span>:d7:<span class="hljs-number">3</span>b:<span class="hljs-number">6</span>c:<span class="hljs-number">3</span>a:bd:<span class="hljs-number">78</span>:<span class="hljs-number">92</span>:<span class="hljs-number">69</span>:<span class="hljs-number">21</span>:<span class="hljs-number">90</span>:<span class="hljs-number">70.</span>
Are you sure you want <span class="hljs-keyword">to</span> <span class="hljs-keyword">continue</span> connecting (yes/no)? yes
Warning: Permanently added <span class="hljs-comment">'Box02,192.168.0.124' (ECDSA) to the list of known hosts.</span>
hadooper@Box02<span class="hljs-comment">'s password: </span>
Now <span class="hljs-keyword">try</span> logging <span class="hljs-keyword">into</span> the machine, <span class="hljs-keyword">with</span> <span class="hljs-string">"ssh 'hadooper@westernseas'"</span>, <span class="hljs-keyword">and</span> check <span class="hljs-keyword">in</span>:

  ~/.ssh/authorized_keys

<span class="hljs-keyword">to</span> make sure we haven<span class="hljs-comment">'t added extra keys that you weren't expecting.</span>
</code></pre>
<p>Following the hint displayed above, I tried to SSH into Box02:</p>
<pre><code class="lang-%7B.code%7D">$ ssh hadooper<span class="hljs-meta">@Box</span>02
Welcome to Ubuntu <span class="hljs-number">11.10</span> (GNU/Linux <span class="hljs-number">3.0</span><span class="hljs-number">.0</span><span class="hljs-number">-16</span>-generic i686)

 * <span class="hljs-string">Documentation:</span>  <span class="hljs-string">https:</span><span class="hljs-comment">//help.ubuntu.com/</span>

<span class="hljs-number">0</span> packages can be updated.
<span class="hljs-number">0</span> updates are security updates.

Last <span class="hljs-string">login:</span> Sun Mar  <span class="hljs-number">4</span> <span class="hljs-number">13</span>:<span class="hljs-number">20</span>:<span class="hljs-number">13</span> <span class="hljs-number">2012</span> from localhost
</code></pre>
<p>The key thing to notice in the code shown above is that that I was never
prompted for a password. That is the way things should be when an SSH
public/private key pair is set up correctly.</p>
<p>Once you have setup SSH so that you can pop over to Box02 without being
prompted for a password, you want to do the same thing in reverse; that
is, you want to set things up so that you can ssh from Box02 to
BoxPrimary. To begin, log into Box02, and then run the same command you
ran in BoxPrimary:</p>
<pre><code class="lang-%7B.code%7D">ssh-copy-id -<span class="hljs-selector-tag">i</span> <span class="hljs-variable">$HOME</span>/.ssh/id_rsa<span class="hljs-selector-class">.pub</span> hadooper@BoxPrimary
</code></pre>
<p>Now check to make sure you can ssh into BoxPrimary without entering a
password:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-attribute">ssh</span> hadooper<span class="hljs-variable">@BoxPrimary</span>
</code></pre>
<a class="anchor" id="setup-master-and-slave-files"></a>
<h3>Setup Master and Slave Files</h3>
<p>You need to edit core-site.xml and madred-site.xml and change the URL
from localhost to BoxPrimary in both BoxPrimary and Box02:</p>
<pre><code><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.default.name<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://BoxPrimary:54310<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>               
</code></pre><p>Make this change also in mapred-site.xml:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapred.job.tracker<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>BoxPrimary:54311<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
</code></pre>
<p>In BoxPrimary, go ahead and start the server</p>
<pre><code class="lang-%7B.code%7D">./<span class="hljs-literal">start</span>-dfs.sh
</code></pre>
<p>In BoxPrimary:</p>
<pre><code class="lang-%7B.code%7D">$ jps
<span class="hljs-number">10581</span> Jps
<span class="hljs-number">10581</span> Jps
<span class="hljs-number">10510</span> SecondaryNam10100 NameNode
</code></pre>
<p>In Box02:</p>
<pre><code class="lang-%7B.code%7D"> <span class="hljs-keyword">jps
</span><span class="hljs-number">6281</span> <span class="hljs-keyword">J5956 </span>DataNode
</code></pre>
<p>If you have trouble, check the logs in Box02:</p>
<pre><code class="lang-%7B.code%7D"> <span class="hljs-built_in">cat</span> hadoop-hadooper-datanode-Box02.<span class="hljs-built_in">log</span>
</code></pre>
<p>Errors can include something like this:  <strong>INFO
org.apache.hadoop.ipc.Client: Retrying connect to server:</strong>Errors of
that type probably means there is something wrong in the /etc/hosts file
either on PrimaryBox or Box02, or perhaps in both places.</p>
<p>If everything is working, then
<a href="http://BoxPrimary:50070/dfshealth.jsp">http://BoxPrimary:50070/dfshealth.jsp</a>
will show at least one live node</p>
<a class="anchor" id="runTest"></a>
<h3>Running a Test</h3>
<p>This script is called <strong>RunApp.sh.</strong>It is designed to run one of the
example applications that ship with hadoop. It is probably a good idea
to completely restart the system between tests, as described above in
the <a href="#restart">Restart</a> section.</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-meta">#!/bin/bash
</span>
HADOOP=<span class="hljs-string">"/usr/local/hadoop/bin/hadoop"</span>
<span class="hljs-built_in">echo</span> <span class="hljs-variable">$HADOOP</span>
DFS=<span class="hljs-variable">$HADOOP</span><span class="hljs-string">" dfs"</span>
JAR=<span class="hljs-variable">$HADOOP</span><span class="hljs-string">" jar"</span>
GUTENBERG=<span class="hljs-string">"/user/hadooper/gutenberg"</span>
OUTPUT=<span class="hljs-variable">$GUTENBERG</span><span class="hljs-string">"-output"</span>

<span class="hljs-built_in">echo</span> <span class="hljs-variable">$DFS</span>
<span class="hljs-variable">$DFS</span> -rmr <span class="hljs-variable">$GUTENBERG</span>
<span class="hljs-variable">$DFS</span> -rmr <span class="hljs-variable">$OUTPUT</span>
<span class="hljs-variable">$DFS</span> -copyFromLocal <span class="hljs-variable">$HOME</span>/gutenberg <span class="hljs-variable">$GUTENBERG</span>
<span class="hljs-variable">$DFS</span> -ls /user/hadooper
<span class="hljs-variable">$DFS</span> -ls <span class="hljs-variable">$GUTENBERG</span>

<span class="hljs-variable">$JAR</span> /usr/<span class="hljs-built_in">local</span>/hadoop/hadoop-examples-1.0.1.jar wordcount <span class="hljs-variable">$GUTENBERG</span> <span class="hljs-variable">$OUTPUT</span>
<span class="hljs-built_in">read</span> -p <span class="hljs-string">"Press [Enter] key to see the results"</span>
/usr/<span class="hljs-built_in">local</span>/hadoop/bin/hadoop dfs -cat /user/hadooper/gutenberg-o/usr/<span class="hljs-built_in">local</span>/hadoop/bin/hadoop dfs -cat /user/hadooper/gutenberg-output/part-r-00000
</code></pre>
<a class="anchor" id="scripts"></a>
<h2>The Scripts</h2>
<p>If you download the <a href="Mercurial.html">mercurial sources</a> linked to
Elvenware you will find a number of the scripts shown on this page in
the Python/CreateHadoopFiles directory. Here is how to use them.</p>
<p>When you download the files from elvenware repository, it is often
helpful to copy the hadoop files into a folder called bin:</p>
<pre><code class="lang-%7B.code%7D">/<span class="hljs-built_in">home</span>/hadooper/bin
</code></pre>
<p>If you are already in <strong>bin</strong>, then you can use this command to copy the
files from the downloaded repsoitory into your current directory, so
long as you have the most recent version of the repository in
<strong>/home/hadooper/andelf</strong>:</p>
<pre><code class="lang-%7B.code%7D">cp <span class="hljs-regexp">/home/</span>hadooper<span class="hljs-regexp">/andelf/</span>Python<span class="hljs-regexp">/CreateHadoopFiles/</span>src<span class="hljs-regexp">/* .</span>
</code></pre>
<p>Then you need to run one or more of the scripts. To prepare them, first
make them executable:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-attribute">chmod</span> +x <span class="hljs-regexp">*.sh</span>
</code></pre>
<p>You can now run the scripts by typing ./SomeScript.sh. For instance, you
might do this to execute the script called GetBooks:</p>
<pre><code class="lang-%7B.code%7D"><span class="hljs-string">./GetBooks.sh</span>
</code></pre>
<p>After running the script, you should find that James Joyce&#39;s <strong>Ulysses</strong>
and other books have been downloaded and stored in the following folder:</p>
<pre><code class="lang-%7B.code%7D">/<span class="hljs-built_in">home</span>/hadooper/gutenberg
</code></pre>
<p>Considered as a whole, the Hadooper scripts from the repository are
designed to to perform certain key steps, such as downloading and
installing Hadoop, or setting up SSH, or running the application.</p>
<p>Here is a good order for running the scripts:</p>
<ul>
<li>Get <a href="#java">Java</a> set up one way or another. The script
<strong>./addSun.sh</strong>will do this for you, but <strong>cat</strong> it out first so you
understand what it does.</li>
<li>Create the user by running <strong>./CreateUsers.sh</strong>.</li>
</ul>
<p>After creating the users will automatically be running as the user
Hadoop. Now:</p>
<ul>
<li>Run the ssh script: <strong>./setupSsh.sh</strong></li>
<li>Run <strong>addNano.sh</strong> which installs hadoop. (Sorry about the naming, I
need to fix it change the name to <strong>addHadoop.sh</strong>. At this stage,
you may need to become the regular user for a moment to be sure that
the newly created <strong>.bashrc</strong> gets run.</li>
<li>Run the python script called <strong>CreateScripts.sh: python
CreateScripts.py</strong></li>
</ul>
<p>There is some manual configuration you will need to do with the host
files and copying the .ssh keys back to the primary or master box. After
that, you are essentially ready to run. I would go through this process
after you bring up all the nodes, and before you first run an
application:</p>
<ul>
<li>On the Master machine, run <strong>./MasterCleanAndRestart.sh</strong></li>
<li>The script will pause halfway through. During the pause, run
./CleanAndRestart.sh on each of the clients.</li>
<li>Come back and press enter to finish running
<strong>./MasterCleanAndRestart.sh</strong></li>
<li>Now you can run the test: <strong>./RunApp.sh</strong></li>
</ul>
<p>As long as you don&#39;t shut down the machines, you shouldn&#39;t have to
bother with running <strong>./MasterCleanAndRestart.sh</strong>
or<strong>./CleanAndRestart.sh</strong> inbetween test runs The problems I&#39;ve had
seem to occur when the file systems get out of sync on the various data
nodes. Once that happens, I think it is simplest to start over, but
below in the Linkx section you can see a note about namespaceids.</p>
<p>The other problem:: I have had the whole process of running the
application stop during the Reduce phase at some point such as 22%.
There is a very long pause, maybe five minutes, and then you get an
error about too many something or others being created, and then the
process finishes normally. When I encountered this error, it meant that
I had one or or more of my hosts files incorrectly configured, by which
I mean I simply had a type in the name of one of the hosts, or had left
out a host, or had typed in an IP address incorrectly. Nothing tricky,
but just wrong.</p>
<a class="anchor" id="links"></a>
<h2>Links</h2>
<ul>
<li><a href="http://wiki.apache.org/hadoop/GettingStartedWithHadoop">http://wiki.apache.org/hadoop/GettingStartedWithHadoop</a></li>
<li><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/">http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/</a></li>
<li><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/#java-io-ioexception-incompatible-namespaceids">java-io-ioexception-incompatible-namespaceids</a></li>
</ul>
<p>Copyright © <a href="../../index.html">Charlie Calvert</a> | <a href="../../index.html">Elvenware
Home</a> | <a href="../index.html">Writing Code</a> |
<a href="../delphi/index.html">Delphi</a> | <a href="../csharp/index.html">CSharp</a> | <a href="../../books/index.html">My
Books</a></p>
</div></body></html>